[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Blackjack is a popular card game played against a dealer. The goal is to have cards whose total value is higher than the dealer’s but not exceeding 21. Each card has a point value: the cards 2 through 10 are worth their face value, face cards (Jack, Queen, King) are worth 10, and an Ace can be worth 1 or 11. The game is typically played with multiple decks of standard playing cards."
  },
  {
    "objectID": "index.html#data-science-question",
    "href": "index.html#data-science-question",
    "title": "Introduction",
    "section": "Data Science Question",
    "text": "Data Science Question\nThe impact of COVID-19 has been profound and multi-faceted, affecting nearly every aspect of life globally. Two of the most significant impacts have been on human life and the global economy.\nCOVID-19 has resulted in a tragic loss of life worldwide. Globally, as of 3:52pm CET, 30 November 2023, there have been 772,052,752 confirmed cases of COVID-19, including 6,985,278 deaths, reported to WHO. This loss has not only been a human tragedy but has also had psychological and social repercussions, affecting the mental health of communities and changing the way people mourn and grieve.\n\nThe pandemic has had a devastating impact on the global economy. To control the spread of the virus, many countries implemented lockdowns and social distancing measures, which led to a dramatic slowdown in economic activities. Key sectors such as travel, hospitality, and retail were particularly hard hit. The economic disruption caused widespread job losses, business closures, and financial strain for individuals and families. Governments around the world have had to inject substantial fiscal stimulus to support their economies, leading to increased national debts.\nOverall, the COVID-19 pandemic has been a defining global crisis of the early 21st century, with its impacts likely to be felt for many years to come. Consequently, I believe that a thorough analysis of the factors affecting COVID-19’s transmission and mortality is imperative. Such an investigation will enhance our comprehension of, and preparedness for, any future global pandemics. By understanding the variables that influence the spread and lethality of such diseases, we can develop more effective strategies to curb their proliferation and minimize fatalities. This proactive approach is vital to safeguard public health and preserve lives."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review",
    "text": "Literature Review\n\nPopulation Density and the Transmission of COVID-19\nThe COVID-19 pandemic has prompted extensive research into factors influencing its spread. A significant area of focus has been the role of population density.\nSy et al. (2021) conducted a comprehensive study across U.S. counties, revealing a strong correlation between population density and the basic reproductive number (R0) of COVID-19. They found that areas with higher population densities experienced greater transmission rates, likely due to increased interpersonal contact. The study identified a critical population density threshold necessary to sustain an outbreak, emphasizing the significance of population density in virus transmission dynamics (Sy et al., 2021).\nIn a study focusing on Turkey, Coşkun et al. (2021) explored the combined effect of population density and wind on COVID-19 spread. They concluded that these factors accounted for a significant portion of the variance in virus transmission. The study highlighted that wind, by increasing air circulation, could exacerbate the spread in denser areas, thus underlining the complex interplay between environmental factors and population density in the pandemic’s trajectory (Coşkun et al., 2021).\nYin et al. (2021) investigated the association between population density and COVID-19 infection rates in both China and the USA. Their findings underscored a positive correlation, particularly in regions with severe outbreaks. The study supported the efficacy of social distancing and travel restrictions, pointing out the critical role of population density in managing the spread of the virus (Yin et al., 2021).\nLastly, Wong and Li (2020) presented a study demonstrating that population density was an effective predictor of cumulative infection cases in U.S. counties. Their research incorporated additional demographic variables, such as the percentages of African Americans and Hispanic-Latinas, finding that while these factors influenced infection rates, the impact of population density remained consistently significant. This study highlighted the necessity of including population density in predictive models for COVID-19 spread (Wong & Li, 2020).\nIn conclusion, these studies collectively underscore the crucial role of population density in the transmission dynamics of COVID-19.\n\n\nVaccination and the Infection/Death Rates of COVID-19\nThe onset of the COVID-19 pandemic has prompted unprecedented global efforts in vaccine development and distribution at the same time. As vaccination campaigns roll out worldwide, it becomes crucial to evaluate their impact on reducing infection and mortality rates.\nA study by the BMJ (2023) conducted a comprehensive observational study to assess the public health impact of COVID-19 vaccines across counties in the United States. Using data from December 2020 to December 2021, the study utilized generalized linear mixed models to explore the association between vaccination coverage and the rates of COVID-19 cases and deaths. The study’s findings indicate a significant reduction in COVID-19 cases and deaths correlating with increased vaccination coverage, even when accounting for social vulnerability and community mobility (The BMJ, 2023).\nSimilarly, another study published in Scientific Reports (2023) analyzed the impact of COVID-19 vaccination on the pandemic’s trajectory in various U.S. states. This study focused on the average treatment effect of vaccination on the growth rates of total cases and hospitalizations. It found that COVID-19 vaccines have significantly slowed the pandemic, with a notable reduction in the number of cases and hospitalizations. This study also explored potential biases and the heterogeneous effects of vaccination across different states, finding no significant differences based on demographic or political factors (Scientific Reports, 2023).\nExpanding the scope to a global perspective, a study indexed in PubMed (2023) analyzed the effect of COVID-19 vaccination on daily cases and deaths worldwide. The study demonstrated that increased vaccination rates are associated with a decrease in new COVID-19 cases and deaths globally. However, it also highlighted the challenges of unequal vaccine distribution across countries, emphasizing the need for fair and accelerated distribution to combat the pandemic effectively (PubMed, 2023).\nCollectively, these studies offer robust evidence of the positive impact of COVID-19 vaccination campaigns on reducing infection and mortality rates."
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nThis study evaluated several reinforcement learning (RL) strategies, including Monte Carlo (MC) on-policy, MC off-policy, SARSA, Q-learning, and Deep Q-Network (DQN), within the blackjack simulation provided by the OpenAI Gym framework. The findings revealed that the on-policy Monte Carlo method yielded the most effective strategy. However, despite this relative success, the expected return for the player remains negative. This outcome underscores the inherent challenge of consistently outperforming the casino over the long term in blackjack."
  },
  {
    "objectID": "Data-Sources.html",
    "href": "Data-Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The dataset used in our study is Face Mask Detection ~12K Images Dataset, which is used for Face Mask Detection Classification with images. The dataset consists of almost 12K images which are almost 328.92MB in size.\n\n\n\nSo Many Faces We Have!"
  },
  {
    "objectID": "Data-Sources.html#our-world-in-data-owid-covid-19-data",
    "href": "Data-Sources.html#our-world-in-data-owid-covid-19-data",
    "title": "Data Sources",
    "section": "",
    "text": "The Our World in Data The dataset used in our study is Face Mask Detection ~12K Images Dataset, which is used for Face Mask Detection Classification with images. The dataset consists of almost 12K images which are almost 328.92MB in size.consists of facial headshots. This focus on the face simplifies the process by minimizing background distractions. However, the photos vary in pixel count and dimensions, requiring a standardization to a uniform resolution of 150x150 pixels.\n\n\n\nClick on the logo to access the sample data of US!"
  },
  {
    "objectID": "Data-Sources.html#key-attributes",
    "href": "Data-Sources.html#key-attributes",
    "title": "Data Sources",
    "section": "Key Attributes",
    "text": "Key Attributes\n\n\n\n\n\n\n\n\nimage_path\nlabel\nlocation\n\n\n\n\ninput/Face Mask Dataset/Test/WithoutMask/2734.png\nWithoutMask\nTest\n\n\ninput/Face Mask Dataset/Test/WithoutMask/4345.png\nWithoutMask\nTest\n\n\ninput/Face Mask Dataset/Test/WithoutMask/4423.png\nWithoutMask\nTest\n\n\ninput/Face Mask Dataset/Test/WithoutMask/2052.png\nWithoutMask\nTest\n\n\ninput/Face Mask Dataset/Test/WithoutMask/3364.png\nWithoutMask\nTest"
  },
  {
    "objectID": "Data-Sources.html#data-cleaning",
    "href": "Data-Sources.html#data-cleaning",
    "title": "Data Sources",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data set provided by OWID is very complete, and I only need to do some simple data cleaning work, including converting column formats and using interpolation to deal with a small number of missing values."
  },
  {
    "objectID": "Reference.html",
    "href": "Reference.html",
    "title": "Reference",
    "section": "",
    "text": "https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/\nhttps://www.kaggle.com/code/angps95/blackjack-strategy-using-reinforcement-learning\nhttps://srome.github.io/Train-A-Neural_Net-To-Play-Black-Jack-With-Q-Learning/\nhttps://cs230.stanford.edu/files_winter_2018/projects/6940282.pdf"
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "The results demonstrate varying efficiencies of different RL methods in the game of Blackjack.\n\n\n\nMethod\nAverage Payout\n\n\n\n\nMC(On-policy)\n-0.04703\n\n\nMC(Off-policy)\n-0.08581\n\n\nSARSA\n-0.17661\n\n\nQ-Learning\n-0.14613\n\n\nDQN\n-0.05446\n\n\n\nThe on-policy Monte Carlo method emerges as the most effective, closely followed by the DQN. The lower performance of off-policy methods in this specific application suggests that the direct learning from actual rewards and states as in on-policy methods aligns better with the game’s dynamics. SARSA and off-policy Q-Learning, while generally robust in many scenarios, appear to struggle with optimizing their policies compared to the more straightforward Monte Carlo approaches in the relatively simple and constrained environment of Blackjack. This analysis underscores the importance of selecting an RL strategy that aligns with the specific characteristics and requirements of the environment to maximize learning efficiency and policy effectiveness.\n\nLimitations\n\nSimplicity of Environment:\n\nThe standard Blackjack environment used in these experiments is relatively simple compared to many real-world problems that feature larger state and action spaces, and more complex dynamics. Consequently, the generalizability of these results to more complex or different types of environments may be limited.\n\nEvaluation Metrics:\n\nThe evaluation based solely on average payouts may not fully capture the strategic depth or the risk management aspects of Blackjack. Other metrics such as drawdown, variance of payouts, and strategic flexibility could provide a more comprehensive evaluation of performance.\n\nPolicy Robustness:\n\nThe robustness of the policies against different Blackjack rule sets (e.g., variations in deck size, dealer behavior) was not tested. An effective strategy under one set of rules might perform poorly under another.\n\n\n\n\nFuture Prospects\n\nAlgorithm Enhancements:\n\nFuture work could explore modifications to these algorithms to better handle exploration, perhaps by integrating methods like Thompson sampling or entropy-based exploration bonuses, which could lead to more robust learning.\n\nCross-Environment Learning:\n\nTesting the algorithms in varied environments or on simulated platforms that better mimic the complexities of real-world decision-making could help improve the robustness and applicability of the findings.\n\nHybrid Models:\n\nCombining the strengths of different approaches, such as integrating the robust state evaluation of DQN with the on-policy advantages of Monte Carlo methods, could yield better performance and faster convergence.\n\nAdvanced Neural Architectures:\n\nEmploying more complex neural networks, such as recurrent neural networks (RNNs) or transformers, might capture the temporal dependencies in more complex versions of Blackjack or similar games better than standard DQNs.\n\nComprehensive Evaluation Framework:\n\nDeveloping a more nuanced evaluation framework that includes multiple performance metrics could provide deeper insights into the strategies’ effectiveness and practical utility."
  },
  {
    "objectID": "Data-Sources.html#face-mask-detection-12k-images-dataset",
    "href": "Data-Sources.html#face-mask-detection-12k-images-dataset",
    "title": "Data Sources",
    "section": "",
    "text": "The dataset used in our study is Face Mask Detection ~12K Images Dataset, which is used for Face Mask Detection Classification with images. The dataset consists of almost 12K images which are almost 328.92MB in size.\n\n\n\nSo Many Faces We Have!"
  },
  {
    "objectID": "Data-Sources.html#data-preprocessing",
    "href": "Data-Sources.html#data-preprocessing",
    "title": "Data Sources",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe data set is divided in training, testing and validation directories.\n\n\n\nTraining Example\n\n\n\n\n\nTesting Example\n\n\n\n\n\nValidation Example\n\n\nThe dataset employed in this study comprises facial headshots, which inherently simplifies the analytical process by minimizing extraneous background distractions. Nevertheless, the images exhibit variability in pixel count and dimensions, necessitating their standardization to a uniform resolution of 150x150 pixels to ensure consistency in the data processing and analysis."
  },
  {
    "objectID": "index.html#related-work",
    "href": "index.html#related-work",
    "title": "Introduction",
    "section": "Related Work",
    "text": "Related Work\nThe pursuit of an optimal blackjack strategy has long captivated a diverse array of scholars including mathematicians, statisticians, and gambling enthusiasts, fueling a wealth of academic and practical research over many decades. The groundbreaking study by Baldwin, Cantey, Maisel, and McDermott in 1956, presented in “The Optimum Strategy in Blackjack,” was the inaugural attempt to apply a rigorous mathematical framework to the game. Their formulation of a basic strategy considerably decreased the house advantage, a methodology that was further refined and widely disseminated by Edward O. Thorp in his 1962 seminal work, “Beat the Dealer” (Thorp, 1962). Thorp introduced the concept of card counting along with sophisticated strategy algorithms, which positioned blackjack as a focal point for both professional gamblers and scholarly research.\nThis pioneering work spurred the development of a plethora of card counting systems from the 1960s to the 1980s, designed to afford players a competitive edge against casinos. Notably, Harvey Dubner’s Hi-Lo strategy, alongside subsequent modifications by figures such as Ken Uston and Stanford Wong, gained traction for their efficacy and user-friendly application.\nAdvancements continued into the 1990s and beyond with the advent of computer simulations, which have played a pivotal role in refining strategies. Researchers like Julian Braun and Peter Griffin leveraged advanced computational tools to simulate countless blackjack hands, thereby enhancing playing techniques, honing betting strategies, and enriching the understanding of the game’s statistical foundations.\nThe new millennium has seen further innovations with the application of reinforcement learning (RL) to optimize blackjack strategies, encompassing a range from basic methodologies to advanced machine learning techniques. A tutorial from Gymnasium Farama elucidates the fundamental concepts and practical application of RL in blackjack, offering an accessible introduction for novices utilizing the OpenAI Gym environment (Gymnasium Farama, n.d.). Concurrently, practical implementations on platforms such as Kaggle have demonstrated the utility of Q-learning based agents that maximize gaming returns, underpinned by detailed performance analytics and evaluations of parameter impacts (Kaggle, n.d.). Additionally, a project from Stanford University explores sophisticated strategies utilizing deep learning and neural networks, showcasing the flexibility and challenges of AI in dynamic gaming contexts (Stanford University, 2018). Collectively, these investigations not only deepen the comprehension of strategic decision-making in gaming contexts but also illustrate the potential of AI to revolutionize established gaming strategies."
  },
  {
    "objectID": "Models.html",
    "href": "Models.html",
    "title": "Models",
    "section": "",
    "text": "Monte Carlo (MC) methods in reinforcement learning are a set of techniques used to estimate the value functions and improve policies based on the average of complete returns (total rewards) from multiple episodes. In the context of a game like blackjack, Monte Carlo methods can be very effective for learning optimal strategies because it is an episodic game with clear beginnings and endings.\nMonte Carlo methods can be categorized into two main types based on the relationship between the policy used to generate behavior (the behavior policy) and the policy being improved (the target policy): on-policy and off-policy methods.\n\n\nOn-policy methods involve learning about and improving the same policy that is used to make decisions and generate data. Essentially, the policy used to interact with the environment is the same policy that is evaluated and improved based on the gathered data.\n\nValue FunctionOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 1-1-1: This figure shows the estimated value for player hands without a usable ace across different dealer showing cards. The x-axis represents the dealer’s card showing, ranging from 2 to 10. The y-axis represents the player’s total hand value, ranging from about 5 to 20. The z-axis shows the estimated value of each state. Higher values (in red) indicate more favorable situations for the player, while lower values (in blue) indicate less favorable situations. Notably, the values decrease sharply as the player’s hand approaches and exceeds 17, reflecting increased risk of busting.\n\n\n\n\n\nFigure 1-1-2: This figure shows the estimated value for player hands with a usable ace. The figure again indicates that higher hand values near 21 yield higher estimated values, demonstrating the strategic benefit of having a flexible ace.\n\n\nBoth graphs effectively highlight how the value of a blackjack hand changes depending on the player’s total and the dealer’s up-card, and how having a usable ace increases the player’s advantage due to the flexibility it provides in hand values. This is because the ace can be valued as 1 or 11, giving the player greater flexibility to avoid busting and improve their hand.\n\n\n\n\n\nFigure 1-2: This figure shows the optimal policy for playing blackjack as derived from on-policy Monte Carlo simulations, categorized by whether the player’s hand includes a usable ace or not. Each chart illustrates recommended actions (hit or stick) for various combinations of the dealer’s showing card and the player’s hand total.\n\n\n\n\n\n\n\nFigure 1-3: This figure shows a series of vertical fluctuations in payouts across the 1000 players. The fluctuations vary around the negative payout zone, confirming that most players end up with a loss on average.\n\n\nThe average payout of -0.04703 after 1000 rounds based on the optimal policy from Monte Carlo (MC) on-policy simulations in blackjack suggests that, although the policy is optimized to increase the player’s chances of winning, it does not eliminate the inherent house edge present in the game. This negative average payout indicates that the player is expected to lose approximately 4.7 cents for every dollar bet over a large number of rounds, highlighting the challenge of overcoming the casino’s advantage even with strategic play.\n\n\n\n\n\n\nOff-policy methods use a different approach by learning about a target policy (the policy of interest) based on the behavior generated by another policy (the behavior policy). This separation allows the agent to learn from exploratory or even random actions that are not necessarily part of the optimal strategy.\n\nOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 2-1: This figure shows the optimal policy for playing blackjack using off-policy Monte Carlo methods.\n\n\n\n\n\n\n\nFigure 2-2: This figure shows a series of vertical fluctuations in payouts across the 1000 players.\n\n\nThe average payout of -0.08581 after 1000 rounds based on the optimal policy from Monte Carlo (MC) off-policy simulations in blackjack indicates that, while the policy aims to optimize outcomes based on a separate exploratory policy, it still results in a net loss for the player. This larger average loss compared to the on-policy simulations (-0.08581 vs. -0.04703) suggests that the off-policy method might be either less efficient in exploiting the nuances of the game or may involve more risk due to the nature of the behavior policy used in its learning process."
  },
  {
    "objectID": "Models.html#related-work",
    "href": "Models.html#related-work",
    "title": "Models",
    "section": "Related Work",
    "text": "Related Work\nConvolutional Neural Networks (CNNs) are effectively used for face mask recognition by leveraging their ability to process pixel data from images and learn hierarchical features. CNNs employ multiple layers of filters to extract features from an image. In the context of masked faces, these networks focus on extracting and learning the visible parts of the face, such as the eyes and forehead. This allows the model to recognize individuals even when lower facial features are obscured by masks, by adapting the training process to focus more on the unmasked regions of the face.\nRecent studies in face mask recognition using CNNs have introduced several advancements. Pann & Lee (2022) have pioneered an attention-based CNN model that employs a convolutional block attention module (CBAM), focusing on enhancing feature recognition particularly around the eye regions, which is critical in scenarios where masks obscure facial features. Concurrently, Zhang et al. (2021) present a development in lightweight CNN architectures aimed at real-time face mask detection, balancing computational efficiency with performance. These studies underscore a critical shift towards adapting facial recognition technologies to accommodate scenarios involving face coverings, highlighting both the innovative approach to feature enhancement and the pragmatic application in real-time environments."
  },
  {
    "objectID": "Models.html#convolutional-neural-networks-model",
    "href": "Models.html#convolutional-neural-networks-model",
    "title": "Models",
    "section": "",
    "text": "We developed our model using a convolutional neural network architecture, starting with a convolutional layer of 64 filters followed by max pooling and batch normalization. This initial setup is complemented by successive convolutional layers with 128 and 256 filters respectively, each followed by max pooling and batch normalization to enhance the model’s learning capability. Regularization is achieved through dropout at a rate of 0.25 after each convolutional block to prevent overfitting. The network concludes with a fully connected layer of 256 neurons, employing a ReLU activation function, and a dropout rate of 0.5 to further enforce regularization. The output layer consists of a single neuron with a sigmoid activation function, making the model suitable for binary classification tasks. The model utilizes the Adam optimizer and is trained with a binary cross-entropy loss function, focusing on binary accuracy as the performance metric.\n\n\n\nAccuracyLoss\n\n\n\n\n\nFigure 1-1: This figure shows the training and validation accuracy of the model across epochs. Initially, the training accuracy starts high and remains quite stable, showing little improvement over time, which suggests the model quickly reached a good performance level on the training data. In contrast, the validation accuracy starts much lower, indicating initial underperformance on unseen data, but it significantly improves by the second epoch and then stabilizes, closely mirroring the training accuracy by the fifth epoch. This could suggest that the model, after some adjustments, is generalizing well without overfitting or underfitting.\n\n\n\n\n\n\n\nFigure 1-2: This figure shows the training and validation loss over the epochs. The training loss starts lower and decreases steadily, reflecting the model’s learning progress on the training data. The validation loss starts very high, indicating poor initial performance on the validation set, but it sharply decreases and eventually stabilizes, though with some fluctuation. This reduction in validation loss alongside a stable, low training loss suggests that the model is becoming more effective and consistent in its predictions as it trains.\n\n\n\n\n\n\n\n\nThe model’s accuracy on the testing dataset is 91.63%. This high level of accuracy suggests that the model is performing well on the given task.\n\n\n\nFigure 2: This figure shows that the model has a high rate of correct predictions for both classes, with a particularly low number of false positives, indicating strong performance in distinguishing class 1. However, there are more false negatives for class 0, which could be an area for improvement in model accuracy."
  },
  {
    "objectID": "Models.html#cnn-model-with-attention-mechanism",
    "href": "Models.html#cnn-model-with-attention-mechanism",
    "title": "Models",
    "section": "CNN Model with Attention Mechanism",
    "text": "CNN Model with Attention Mechanism\nWe integrated two types of attention mechanisms into the Convolutional Neural Network (CNN) model:\n\nChannel Attention Mechanism:\n\nThe ChannelAttention layer works by focusing on the ‘what’ of the input data, which means it helps the model pay more attention to the most informative features across the channel dimension. This layer performs the following steps:\n\nAverage Pooling and Max Pooling: These operations are applied to the input tensor along the spatial dimensions (width and height), resulting in two different context descriptors: one for the average and one for the maximum values.\nShared Dense Layers: Two shared dense layers transform these context descriptors. The first dense layer with a relu activation reduces dimensionality by a factor defined by ratio, which controls the trade-off between complexity and performance. The second dense layer restores the dimensions.\nActivation Function: The outputs of the dense layers from the average-pooled and max-pooled paths are added and passed through a sigmoid activation function to produce a scaling factor between 0 and 1.\n\nThese steps generate a channel-wise attention map which is then multiplied by the original input tensor to scale the channel features by their importance.\n\nSpatial Attention Mechanism:\n\nThe SpatialAttention layer, on the other hand, focuses on the ‘where’ of the input data, directing the model’s focus to important spatial locations:\n\nPooling Operations: Average and max pooling operations are applied along the channel axis to produce two separate feature maps, highlighting where significant activations occur across all channels.\nConcatenation and Convolution: The feature maps are concatenated and passed through a convolutional layer with a single filter and a sigmoid activation. This filter generates a spatial attention map, which is learned based on the concatenated contexts of average and max signals.\n\nThe spatial attention map emphasizes specific areas in the input tensor, enabling the model to focus more on salient parts of the image.\nBoth attention mechanisms are integrated after batch normalization and before the dropout layer in each convolutional block of the model. This placement ensures that the attention modules can refine the feature representation post-normalization and before any feature dropout occurs, maintaining the focus on important features throughout the training. By implementing these attention layers, the CNN model becomes more adaptive and focused, improving its ability to recognize patterns, especially in complex visual tasks such as recognizing faces with masks. The attention mechanisms can help mitigate issues like the variance in mask types, positions, and the occlusion they cause by directing the model to focus more on the visible, unoccluded parts of the face.\n\nTraining Process\n\nAccuracyLoss\n\n\n\n\n\nFigure 3-1: This figure shows the training and validation accuracy of the model across six epochs after integrating the attention mechanism. The training accuracy begins high and remains consistent, suggesting the model achieves strong performance on the training data early. The validation accuracy initially shows underperformance but improves dramatically around the third epoch, leveling out to closely match the training accuracy by the sixth epoch. This stability in both training and validation accuracy suggests effective generalization and indicates that the model is well-tuned to the complexities of the task without overfitting.\n\n\n\n\n\n\n\nFigure 3-2: This figure shows the training and validation loss over six epochs. The training loss decreases rapidly and stabilizes at a low level from the third epoch onward, which is indicative of effective learning. The validation loss, although starting higher, mirrors this trend by the third epoch and closely follows the training loss, showing a reduction to a similar low level. This convergence of training and validation losses at low values signifies that the model, enhanced with attention mechanisms, is consistent and robust against overfitting.\n\n\n\n\n\nComparing these results with the previous plots without the attention mechanism, it is evident that the introduction of attention layers has improved the model’s ability to generalize as shown by the reduced gap between training and validation accuracy and loss. Previously, there were significant drops and spikes in validation accuracy and loss, suggesting potential issues with model stability and generalization. With the attention mechanism, these issues seem to have been mitigated, leading to a more stable and reliable model performance across epochs. This improvement likely results from the model’s enhanced capability to focus on the most informative features, reducing the noise and the effects of irrelevant data variations during training.\n\n\nTesting Results\nThe accuracy on the testing data for the CNN model with the attention mechanism is 96.37%, showing a significant improvement over the previous accuracy of 91.63%. This enhancement can be attributed both to the incorporation of the attention mechanism and the observed changes in the confusion matrix.\n\n\n\nFigure 4: This figure represents a confusion matrix for the CNN model enhanced with an attention mechanism, showing the number of correct and incorrect predictions for two classes (0 and 1) after testing.\n\n\nComparing this confusion matrix to the previous one without the attention mechanism, there is a notable improvement in both the false negatives and false positives rates. Previously, the model had a higher number of false negatives for class 0 (82 instances) and only one false positive. In the updated model with attention mechanisms, the number of false negatives dramatically decreased to 3, significantly improving the sensitivity of the model towards class 0. The false positives slightly increased from 1 to 33, indicating a slight trade-off where the model has become more cautious about incorrectly classifying class 1 but at the cost of occasionally misclassifying class 0 as class 1.\nThis enhanced confusion matrix suggests that the attention mechanism has successfully increased the model’s ability to focus on more discriminative features for both classes, thereby reducing the overall error rates and specifically the rate at which the model misses class 0 instances. However, the increase in false positives indicates that while the model is less likely to miss class 0 instances, it might be overcompensating, leading to more class 1 instances being incorrectly classified as class 0. This could be an area for further refinement in balancing the sensitivity and specificity of the model."
  },
  {
    "objectID": "Models.html#comparison",
    "href": "Models.html#comparison",
    "title": "Models",
    "section": "Comparison",
    "text": "Comparison"
  },
  {
    "objectID": "Models.html#summary",
    "href": "Models.html#summary",
    "title": "Models",
    "section": "Summary",
    "text": "Summary\nOverall, the results indicate that the CNN model with attention mechanism is a well-balanced model that is both more sensitive and just slightly less specific, resulting in higher overall accuracy and improved performance on the testing dataset. This makes the model more reliable for practical applications, where high accuracy is critical."
  },
  {
    "objectID": "index.html#environment",
    "href": "index.html#environment",
    "title": "Introduction",
    "section": "Environment",
    "text": "Environment\nThe modeling of blackjack has been buil within the Gym environment. This significantly enhances the accessibility and flexibility for researchers conducting various experiments. This integration into the Gym framework streamlines the process of testing and developing different strategies by providing a standardized model. Here, I will outline the fundamental concepts of this model to aid readers in understanding how it functions and facilitates research in this domain.\n\n\nAction Space\nThe action shape is (1,) in the range {0, 1} indicating whether to stick or hit.\n\n0: Stick\n1: Hit\n\n\n\nObservation Space\nThe observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1). The observation is returned as (int(), int(), int()).\n\n\n\nObservation\nMin\nMax\n\n\n\n\nPlayer current sum\n12\n21\n\n\nDealer showing card value\nA\n10\n\n\nUsable Ace\n0\n1\n\n\n\nWe have set the minimum for the ‘Player current sum’ to 12 in our model, as strategic decision-making between hit or stick becomes relevant only when the player’s sum is 12 or higher. Below this threshold, the player faces no risk of busting and will invariably choose to hit. This assumption simplifies the model without sacrificing its strategic depth.\n\n\nRewards\n\nwin game: +1\nlose game: -1\ndraw game: 0\nwin game with natural blackjack: +1.5 (if natural is True) +1 (if natural is False)\n\n\n\nTermination\nThe episode ends if the following happens:\n\nThe player hits and the sum of hand exceeds 21.\nThe player sticks.\n\nAn ace will always be counted as usable (11) unless it busts the player."
  },
  {
    "objectID": "Models.html#monte-carlo-method",
    "href": "Models.html#monte-carlo-method",
    "title": "Models",
    "section": "",
    "text": "Monte Carlo (MC) methods in reinforcement learning are a set of techniques used to estimate the value functions and improve policies based on the average of complete returns (total rewards) from multiple episodes. In the context of a game like blackjack, Monte Carlo methods can be very effective for learning optimal strategies because it is an episodic game with clear beginnings and endings.\nMonte Carlo methods can be categorized into two main types based on the relationship between the policy used to generate behavior (the behavior policy) and the policy being improved (the target policy): on-policy and off-policy methods.\n\n\nOn-policy methods involve learning about and improving the same policy that is used to make decisions and generate data. Essentially, the policy used to interact with the environment is the same policy that is evaluated and improved based on the gathered data.\n\nValue FunctionOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 1-1-1: This figure shows the estimated value for player hands without a usable ace across different dealer showing cards. The x-axis represents the dealer’s card showing, ranging from 2 to 10. The y-axis represents the player’s total hand value, ranging from about 5 to 20. The z-axis shows the estimated value of each state. Higher values (in red) indicate more favorable situations for the player, while lower values (in blue) indicate less favorable situations. Notably, the values decrease sharply as the player’s hand approaches and exceeds 17, reflecting increased risk of busting.\n\n\n\n\n\nFigure 1-1-2: This figure shows the estimated value for player hands with a usable ace. The figure again indicates that higher hand values near 21 yield higher estimated values, demonstrating the strategic benefit of having a flexible ace.\n\n\nBoth graphs effectively highlight how the value of a blackjack hand changes depending on the player’s total and the dealer’s up-card, and how having a usable ace increases the player’s advantage due to the flexibility it provides in hand values. This is because the ace can be valued as 1 or 11, giving the player greater flexibility to avoid busting and improve their hand.\n\n\n\n\n\nFigure 1-2: This figure shows the optimal policy for playing blackjack as derived from on-policy Monte Carlo simulations, categorized by whether the player’s hand includes a usable ace or not. Each chart illustrates recommended actions (hit or stick) for various combinations of the dealer’s showing card and the player’s hand total.\n\n\n\n\n\n\n\nFigure 1-3: This figure shows a series of vertical fluctuations in payouts across the 1000 players. The fluctuations vary around the negative payout zone, confirming that most players end up with a loss on average.\n\n\nThe average payout of -0.04703 after 1000 rounds based on the optimal policy from Monte Carlo (MC) on-policy simulations in blackjack suggests that, although the policy is optimized to increase the player’s chances of winning, it does not eliminate the inherent house edge present in the game. This negative average payout indicates that the player is expected to lose approximately 4.7 cents for every dollar bet over a large number of rounds, highlighting the challenge of overcoming the casino’s advantage even with strategic play.\n\n\n\n\n\n\nOff-policy methods use a different approach by learning about a target policy (the policy of interest) based on the behavior generated by another policy (the behavior policy). This separation allows the agent to learn from exploratory or even random actions that are not necessarily part of the optimal strategy.\n\nOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 2-1: This figure shows the optimal policy for playing blackjack using off-policy Monte Carlo methods.\n\n\n\n\n\n\n\nFigure 2-2: This figure shows a series of vertical fluctuations in payouts across the 1000 players.\n\n\nThe average payout of -0.08581 after 1000 rounds based on the optimal policy from Monte Carlo (MC) off-policy simulations in blackjack indicates that, while the policy aims to optimize outcomes based on a separate exploratory policy, it still results in a net loss for the player. This larger average loss compared to the on-policy simulations (-0.08581 vs. -0.04703) suggests that the off-policy method might be either less efficient in exploiting the nuances of the game or may involve more risk due to the nature of the behavior policy used in its learning process."
  },
  {
    "objectID": "Models.html#td",
    "href": "Models.html#td",
    "title": "Models",
    "section": "TD",
    "text": "TD\nWe integrated two types of attention mechanisms into the Convolutional Neural Network (CNN) model:\n\nChannel Attention Mechanism:\n\nThe ChannelAttention layer works by focusing on the ‘what’ of the input data, which means it helps the model pay more attention to the most informative features across the channel dimension. This layer performs the following steps:\n\nAverage Pooling and Max Pooling: These operations are applied to the input tensor along the spatial dimensions (width and height), resulting in two different context descriptors: one for the average and one for the maximum values.\nShared Dense Layers: Two shared dense layers transform these context descriptors. The first dense layer with a relu activation reduces dimensionality by a factor defined by ratio, which controls the trade-off between complexity and performance. The second dense layer restores the dimensions.\nActivation Function: The outputs of the dense layers from the average-pooled and max-pooled paths are added and passed through a sigmoid activation function to produce a scaling factor between 0 and 1.\n\nThese steps generate a channel-wise attention map which is then multiplied by the original input tensor to scale the channel features by their importance.\n\nSpatial Attention Mechanism:\n\nThe SpatialAttention layer, on the other hand, focuses on the ‘where’ of the input data, directing the model’s focus to important spatial locations:\n\nPooling Operations: Average and max pooling operations are applied along the channel axis to produce two separate feature maps, highlighting where significant activations occur across all channels.\nConcatenation and Convolution: The feature maps are concatenated and passed through a convolutional layer with a single filter and a sigmoid activation. This filter generates a spatial attention map, which is learned based on the concatenated contexts of average and max signals.\n\nThe spatial attention map emphasizes specific areas in the input tensor, enabling the model to focus more on salient parts of the image.\nBoth attention mechanisms are integrated after batch normalization and before the dropout layer in each convolutional block of the model. This placement ensures that the attention modules can refine the feature representation post-normalization and before any feature dropout occurs, maintaining the focus on important features throughout the training. By implementing these attention layers, the CNN model becomes more adaptive and focused, improving its ability to recognize patterns, especially in complex visual tasks such as recognizing faces with masks. The attention mechanisms can help mitigate issues like the variance in mask types, positions, and the occlusion they cause by directing the model to focus more on the visible, unoccluded parts of the face.\n\nTraining Process\n\nAccuracyLoss\n\n\n\n\n\nFigure 3-1: This figure shows the training and validation accuracy of the model across six epochs after integrating the attention mechanism. The training accuracy begins high and remains consistent, suggesting the model achieves strong performance on the training data early. The validation accuracy initially shows underperformance but improves dramatically around the third epoch, leveling out to closely match the training accuracy by the sixth epoch. This stability in both training and validation accuracy suggests effective generalization and indicates that the model is well-tuned to the complexities of the task without overfitting.\n\n\n\n\n\n\n\nFigure 3-2: This figure shows the training and validation loss over six epochs. The training loss decreases rapidly and stabilizes at a low level from the third epoch onward, which is indicative of effective learning. The validation loss, although starting higher, mirrors this trend by the third epoch and closely follows the training loss, showing a reduction to a similar low level. This convergence of training and validation losses at low values signifies that the model, enhanced with attention mechanisms, is consistent and robust against overfitting.\n\n\n\n\n\nComparing these results with the previous plots without the attention mechanism, it is evident that the introduction of attention layers has improved the model’s ability to generalize as shown by the reduced gap between training and validation accuracy and loss. Previously, there were significant drops and spikes in validation accuracy and loss, suggesting potential issues with model stability and generalization. With the attention mechanism, these issues seem to have been mitigated, leading to a more stable and reliable model performance across epochs. This improvement likely results from the model’s enhanced capability to focus on the most informative features, reducing the noise and the effects of irrelevant data variations during training.\n\n\nTesting Results\nThe accuracy on the testing data for the CNN model with the attention mechanism is 96.37%, showing a significant improvement over the previous accuracy of 91.63%. This enhancement can be attributed both to the incorporation of the attention mechanism and the observed changes in the confusion matrix.\n\n\n\nFigure 4: This figure represents a confusion matrix for the CNN model enhanced with an attention mechanism, showing the number of correct and incorrect predictions for two classes (0 and 1) after testing.\n\n\nComparing this confusion matrix to the previous one without the attention mechanism, there is a notable improvement in both the false negatives and false positives rates. Previously, the model had a higher number of false negatives for class 0 (82 instances) and only one false positive. In the updated model with attention mechanisms, the number of false negatives dramatically decreased to 3, significantly improving the sensitivity of the model towards class 0. The false positives slightly increased from 1 to 33, indicating a slight trade-off where the model has become more cautious about incorrectly classifying class 1 but at the cost of occasionally misclassifying class 0 as class 1.\nThis enhanced confusion matrix suggests that the attention mechanism has successfully increased the model’s ability to focus on more discriminative features for both classes, thereby reducing the overall error rates and specifically the rate at which the model misses class 0 instances. However, the increase in false positives indicates that while the model is less likely to miss class 0 instances, it might be overcompensating, leading to more class 1 instances being incorrectly classified as class 0. This could be an area for further refinement in balancing the sensitivity and specificity of the model."
  },
  {
    "objectID": "Models.html#temporal-difference-learning-method",
    "href": "Models.html#temporal-difference-learning-method",
    "title": "Models",
    "section": "Temporal Difference Learning Method",
    "text": "Temporal Difference Learning Method\nTemporal Difference (TD) Learning is a central method in reinforcement learning that combines ideas from Monte Carlo methods and dynamic programming. Unlike Monte Carlo methods, which wait until the end of an episode to update value estimates, TD learning updates estimates incrementally at each step of an episode. This allows TD learning to be used even in non-terminating (continuous) environments. TD methods bootstrap, which means they update value estimates based on other estimated values rather than waiting for a final outcome.\n\nSARSA (On-Policy TD Learning)\nSARSA, which stands for State-Action-Reward-State-Action, is an on-policy TD learning algorithm. It updates the action-value function based on the action taken by the current policy, hence being described as on-policy. This method focuses on evaluating and improving the policy that is used to make decisions, with the learning directly influenced by the actions taken according to the policy.\n\nOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 3-1: This figure shows the optimal policy for playing blackjack using SARSA methods.\n\n\n\n\n\n\n\nFigure 3-2: This figure shows a series of vertical fluctuations in payouts across the 1000 players.\n\n\nThe average payout after 1000 rounds based on the optimal policy from SARSA in blackjack is -0.17661.\n\n\n\n\n\nQ-Learning (Off-Policy TD Learning)\nQ-learning is an off-policy TD learning algorithm and one of the most popular methods in reinforcement learning. It aims to find the optimal policy even when actions are taken according to an exploratory or different behavior policy.\n\nOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 4-1: This figure shows the optimal policy for playing blackjack using Q-learning methods.\n\n\n\n\n\n\n\nFigure 4-2: This figure shows a series of vertical fluctuations in payouts across the 1000 players.\n\n\nThe average payout after 1000 rounds based on the optimal policy from Q-Learning in blackjack is -0.14613."
  },
  {
    "objectID": "Models.html#deep-q-learning-method",
    "href": "Models.html#deep-q-learning-method",
    "title": "Models",
    "section": "Deep Q-Learning Method",
    "text": "Deep Q-Learning Method\nDeep Q-Learning is an advanced reinforcement learning technique that combines Q-learning, a model-free reinforcement learning algorithm, with deep neural networks. This approach is used to learn the optimal action-selection policy for a given task by using a neural network as a function approximator.\nIn our Deep Q-Network (DQN) configuration for Blackjack, we employ a discount factor of 0.99, which sufficiently prioritizes future rewards while ensuring that immediate outcomes remain influential in decision-making. This is appropriate given that Blackjack games generally reach a conclusion after a few moves. After experimenting with various learning rates, we opted for a rate of 0.0001. This rate struck a balance between efficient learning and stability; lower rates such as 0.00001 and 0.000001 resulted in excessively slow training progress, even in relatively small networks. The training regimen involved 500,000 episodes to robustly optimize its decision-making capabilities.\n\nTraining ProcessOptimal PolicyAverage Payoff\n\n\n\n\n\nFigure 5-1: This figure shows the evolution of the loss during the training of the Deep Q-Network over 500,000 episodes.\n\n\n\n\n\n\n\nFigure 5-2: This figure shows the optimal policy for playing blackjack using DQN methods.\n\n\n\n\n\n\n\nFigure 5-3: This figure shows a series of vertical fluctuations in payouts across the 1000 players.\n\n\nThe average payout after 1000 rounds based on the optimal policy from Q-Learning in blackjack is -0.05446."
  }
]